{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DS303-Project-v1.0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UTuvUYncz6Z",
        "outputId": "18eb63c1-d548-4c3a-fb9f-f4ede17affed"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0R6MqEibQ4UF",
        "outputId": "d238411c-8686-494d-f810-b32dcc99d3b8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCTMDAMbtQZs",
        "outputId": "21049d84-a971-4331-cccb-9ffbeaf0aae0"
      },
      "source": [
        "% cd \"/content/drive/MyDrive/DS 303 Project\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/DS 303 Project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oimJjWFQBZRc"
      },
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJmx0otSC4zN",
        "outputId": "8553e9e5-31e3-42b8-fc50-4af3119a5df0"
      },
      "source": [
        "#Reading dataset\n",
        "dataset = pd.read_csv(\"transliteration.txt\",delimiter = \"\\t\",header=None,encoding='utf-8',na_filter = False)\n",
        "\n",
        "#Splitting English words in X and Hindi words in y\n",
        "x = dataset.iloc[:,0]\n",
        "y = dataset.iloc[:,-1]\n",
        "\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0        khushboo\n",
            "1        khushbuu\n",
            "2         khushbu\n",
            "3         khusbhu\n",
            "4            tera\n",
            "           ...   \n",
            "30818      jaandi\n",
            "30819       erade\n",
            "30820        lewa\n",
            "30821    chaawain\n",
            "30822     jattiya\n",
            "Name: 0, Length: 30823, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "If7k-FxyeHD1"
      },
      "source": [
        "with open('train.rel.2.en.txt', 'w') as f:\n",
        "  for i in x[:24000]:\n",
        "    for j in i:\n",
        "      f.write(j + ' ')\n",
        "    f.write('\\n')\n",
        "\n",
        "with open('train.rel.2.hn.txt', 'w') as f:\n",
        "  for i in y[:24000]:\n",
        "    for j in i:\n",
        "      f.write(j + ' ')\n",
        "    f.write('\\n')\n",
        "\n",
        "with open('test.rel.2.en.txt', 'w') as f:\n",
        "  for i in x[24000:]:\n",
        "    for j in i:\n",
        "      f.write(j + ' ')\n",
        "    f.write('\\n')\n",
        "\n",
        "with open('test.rel.2.hn.txt', 'w') as f:\n",
        "  for i in y[24000:]:\n",
        "    for j in i:\n",
        "      f.write(j + ' ')\n",
        "    f.write('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQsZY5CrfSLl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZKfJv2SXJjQ"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "import gzip\n",
        "import os\n",
        "import re\n",
        "import tarfile\n",
        "\n",
        "from six.moves import urllib\n",
        "\n",
        "from tensorflow.python.platform import gfile\n",
        "\n",
        "# Special vocabulary symbols - we always put them at the start.\n",
        "_PAD = b\"_PAD\"\n",
        "_GO = b\"_GO\"\n",
        "_EOS = b\"_EOS\"\n",
        "_UNK = b\"_UNK\"\n",
        "_START_VOCAB = [_PAD, _GO, _EOS, _UNK]\n",
        "\n",
        "PAD_ID = 0\n",
        "GO_ID = 1\n",
        "EOS_ID = 2\n",
        "UNK_ID = 3\n",
        "\n",
        "# Regular expressions used to tokenize.\n",
        "_CHAR_SPLIT = re.compile(b\"([.,!?\\\"':;)(])\")\n",
        "_DIGIT_RE = re.compile(br\"\\d\")\n",
        "\n",
        "\n",
        "def maybe_download(directory, filename, url):\n",
        "  \"\"\"Download filename from url unless it's already in directory.\"\"\"\n",
        "  if not os.path.exists(directory):\n",
        "    print(\"Creating directory %s\" % directory)\n",
        "    os.mkdir(directory)\n",
        "  filepath = os.path.join(directory, filename)\n",
        "  if not os.path.exists(filepath):\n",
        "    print(\"Downloading %s to %s\" % (url, filepath))\n",
        "    filepath, _ = urllib.request.urlretrieve(url, filepath)\n",
        "    statinfo = os.stat(filepath)\n",
        "    print(\"Succesfully downloaded\", filename, statinfo.st_size, \"bytes\")\n",
        "  return filepath\n",
        "\n",
        "\n",
        "def gunzip_file(gz_path, new_path):\n",
        "  \"\"\"Unzips from gz_path into new_path.\"\"\"\n",
        "  print(\"Unpacking %s to %s\" % (gz_path, new_path))\n",
        "  with gzip.open(gz_path, \"rb\") as gz_file:\n",
        "    with open(new_path, \"wb\") as new_file:\n",
        "      for line in gz_file:\n",
        "        new_file.write(line)\n",
        "\n",
        "\n",
        "def get_rev_enhn_train_set(directory):\n",
        "  \"\"\"Check whether training files exist\"\"\"\n",
        "  print(directory)\n",
        "  train_path = os.path.join(directory, \"train.rel.2\")\n",
        "  if not (gfile.Exists(train_path +\".hn.txt\") and gfile.Exists(train_path +\".en.txt\")):\n",
        "    raise ValueError(\"Training files %s not found.\", train_path)\n",
        "  return train_path\n",
        "\n",
        "\n",
        "def get_rev_enhn_dev_set(directory):\n",
        "  \"\"\"heck whether Development files exist.\"\"\"\n",
        "  dev_name = \"test.rel.2\"\n",
        "  dev_path = os.path.join(directory, dev_name)\n",
        "  if not (gfile.Exists(dev_path + \".hn.txt\") and gfile.Exists(dev_path + \".en.txt\")):\n",
        "    raise ValueError(\"Devlopment files %s not found.\", dev_path)\n",
        "  return dev_path\n",
        "\n",
        "\n",
        "def basic_tokenizer(sentence):\n",
        "  \"\"\"Very basic tokenizer: split the word into a list of tokens.\"\"\"\n",
        "  words = []\n",
        "  for space_separated_fragment in sentence.strip().split():\n",
        "    words.extend(re.split(_CHAR_SPLIT, space_separated_fragment))\n",
        "  return [w for w in words if w]\n",
        "\n",
        "\n",
        "def create_vocabulary(vocabulary_path, data_path, max_vocabulary_size,\n",
        "                      tokenizer=None, normalize_digits=True):\n",
        "  \"\"\"Create vocabulary file (if it does not exist yet) from data file.\n",
        "  Data file is assumed to contain one word per line. Each word is\n",
        "  tokenized and digits are normalized (if normalize_digits is set).\n",
        "  Vocabulary contains the most-frequent tokens up to max_vocabulary_size.\n",
        "  We write it to vocabulary_path in a one-token-per-line format, so that later\n",
        "  token in the first line gets id=0, second line gets id=1, and so on.\n",
        "  Args:\n",
        "    vocabulary_path: path where the vocabulary will be created.\n",
        "    data_path: data file that will be used to create vocabulary.\n",
        "    max_vocabulary_size: limit on the size of the created vocabulary.\n",
        "    tokenizer: a function to use to tokenize each data sentence;\n",
        "      if None, basic_tokenizer will be used.\n",
        "    normalize_digits: Boolean; if true, all digits are replaced by 0s.\n",
        "  \"\"\"\n",
        "  if not gfile.Exists(vocabulary_path):\n",
        "    print(\"Creating vocabulary %s from data %s\" % (vocabulary_path, data_path))\n",
        "    vocab = {}\n",
        "    with gfile.GFile(data_path, mode=\"rb\") as f:\n",
        "      counter = 0\n",
        "      for line in f:\n",
        "        counter += 1\n",
        "        if counter % 10000 == 0:\n",
        "          print(\"  processing line %d\" % counter)\n",
        "        tokens = tokenizer(line) if tokenizer else basic_tokenizer(line.lower())\n",
        "        for w in tokens:\n",
        "          word = re.sub(_DIGIT_RE, b\"0\", w) if normalize_digits else w\n",
        "          if word in vocab:\n",
        "            vocab[word] += 1\n",
        "          else:\n",
        "            vocab[word] = 1\n",
        "      vocab_list = _START_VOCAB + sorted(vocab, key=vocab.get, reverse=True)\n",
        "      if len(vocab_list) > max_vocabulary_size:\n",
        "        vocab_list = vocab_list[:max_vocabulary_size]\n",
        "      with gfile.GFile(vocabulary_path, mode=\"wb\") as vocab_file:\n",
        "        for w in vocab_list:\n",
        "          vocab_file.write(w + b\"\\n\")\n",
        "\n",
        "\n",
        "def initialize_vocabulary(vocabulary_path):\n",
        "  \"\"\"Initialize vocabulary from file.\n",
        "  We assume the vocabulary is stored one-item-per-line, so a file:\n",
        "    d\n",
        "    c\n",
        "  will result in a vocabulary {\"d\": 0, \"c\": 1}, and this function will\n",
        "  also return the reversed-vocabulary [\"d\", \"c\"].\n",
        "  Args:\n",
        "    vocabulary_path: path to the file containing the vocabulary.\n",
        "  Returns:\n",
        "    a pair: the vocabulary (a dictionary mapping string to integers), and\n",
        "    the reversed vocabulary (a list, which reverses the vocabulary mapping).\n",
        "  Raises:\n",
        "    ValueError: if the provided vocabulary_path does not exist.\n",
        "  \"\"\"\n",
        "  if gfile.Exists(vocabulary_path):\n",
        "    rev_vocab = []\n",
        "    with gfile.GFile(vocabulary_path, mode=\"rb\") as f:\n",
        "      rev_vocab.extend(f.readlines())\n",
        "    rev_vocab = [line.strip() for line in rev_vocab]\n",
        "    vocab = dict([(x, y) for (y, x) in enumerate(rev_vocab)])\n",
        "    return vocab, rev_vocab\n",
        "  else:\n",
        "    raise ValueError(\"Vocabulary file %s not found.\", vocabulary_path)\n",
        "\n",
        "\n",
        "def word_to_token_ids(word, vocabulary,\n",
        "                          tokenizer=None, normalize_digits=True):\n",
        "  \"\"\"Convert a string to list of integers representing token-ids.\n",
        "  For example, a word \"dog\" may become tokenized into\n",
        "  [\"d\" , \"o\" , \"g\"] and with vocabulary {\"d\": 0, \"o\": 1,\n",
        "  \"g\": 2} this function will return [0,1,2].\n",
        "  Args:\n",
        "    word: the word in bytes format to convert to token-ids.\n",
        "    vocabulary: a dictionary mapping tokens to integers.\n",
        "    tokenizer: a function to use to tokenize each word;\n",
        "      if None, basic_tokenizer will be used.\n",
        "    normalize_digits: Boolean; if true, all digits are replaced by 0s.\n",
        "  Returns:\n",
        "    a list of integers, the token-ids for the word.\n",
        "  \"\"\"\n",
        "\n",
        "  if tokenizer:\n",
        "    chars = tokenizer(word)\n",
        "  else:\n",
        "    chars = basic_tokenizer(word)\n",
        "  if not normalize_digits:\n",
        "    return [vocabulary.get(w, UNK_ID) for w in chars]\n",
        "  # Normalize digits by 0 before looking chars up in the vocabulary.\n",
        "  return [vocabulary.get(re.sub(_DIGIT_RE, b\"0\", w), UNK_ID) for w in chars]\n",
        "\n",
        "\n",
        "def data_to_token_ids(data_path, target_path, vocabulary_path,\n",
        "                      tokenizer=None, normalize_digits=True):\n",
        "  \"\"\"Tokenize data file and turn into token-ids using given vocabulary file.\n",
        "  This function loads data line-by-line from data_path, calls the above\n",
        "  word_to_token_ids, and saves the result to target_path. See comment\n",
        "  for word_to_token_ids on the details of token-ids format.\n",
        "  Args:\n",
        "    data_path: path to the data file in one-word-per-line format.\n",
        "    target_path: path where the file with token-ids will be created.\n",
        "    vocabulary_path: path to the vocabulary file.\n",
        "    tokenizer: a function to use to tokenize each sentence;\n",
        "      if None, basic_tokenizer will be used.\n",
        "    normalize_digits: Boolean; if true, all digits are replaced by 0s.\n",
        "  \"\"\"\n",
        "  if not gfile.Exists(target_path):\n",
        "    print(\"Tokenizing data in %s\" % data_path)\n",
        "    vocab, _ = initialize_vocabulary(vocabulary_path)\n",
        "    with gfile.GFile(data_path, mode=\"rb\") as data_file:\n",
        "      with gfile.GFile(target_path, mode=\"w\") as tokens_file:\n",
        "        counter = 0\n",
        "        for line in data_file:\n",
        "          counter += 1\n",
        "          if counter % 10000 == 0:\n",
        "            print(\"  tokenizing line %d\" % counter)\n",
        "          token_ids = word_to_token_ids(line, vocab, tokenizer,\n",
        "                                            normalize_digits)\n",
        "          tokens_file.write(\" \".join([str(tok) for tok in token_ids]) + \"\\n\")\n",
        "\n",
        "\n",
        "def prepare_rev_data(data_dir, en_vocabulary_size, hn_vocabulary_size, tokenizer=None):\n",
        "  \"\"\"Get REV data into data_dir, create vocabularies and tokenize data.\n",
        "  Args:\n",
        "    data_dir: directory in which the data sets will be stored.\n",
        "    en_vocabulary_size: size of the English vocabulary to create and use.\n",
        "    hn_vocabulary_size: size of the Hindi vocabulary to create and use.\n",
        "    tokenizer: a function to use to tokenize each data sentence;\n",
        "      if None, basic_tokenizer will be used.\n",
        "  Returns:\n",
        "    A tuple of 6 elements:\n",
        "      (1) path to the token-ids for English training data-set,\n",
        "      (2) path to the token-ids for Hindi training data-set,\n",
        "      (3) path to the token-ids for English development data-set,\n",
        "      (4) path to the token-ids for Hindi development data-set,\n",
        "      (5) path to the English vocabulary file,\n",
        "      (6) path to the Hindi vocabulary file.\n",
        "  \"\"\"\n",
        "  # Get REV data to the specified directory.\n",
        "  train_path = get_rev_enhn_train_set(data_dir)\n",
        "  dev_path = get_rev_enhn_dev_set(data_dir)\n",
        "\n",
        "  # Create vocabularies of the appropriate sizes.\n",
        "  hn_vocab_path = os.path.join(data_dir, \"vocab%d.hn\" % hn_vocabulary_size)\n",
        "  en_vocab_path = os.path.join(data_dir, \"vocab%d.en\" % en_vocabulary_size)\n",
        "  create_vocabulary(hn_vocab_path, train_path + \".hn.txt\", hn_vocabulary_size, tokenizer)\n",
        "  create_vocabulary(en_vocab_path, train_path + \".en.txt\", en_vocabulary_size, tokenizer)\n",
        "\n",
        "  # Create token ids for the training data.\n",
        "  hn_train_ids_path = train_path + (\".ids%d.hn\" % hn_vocabulary_size)\n",
        "  en_train_ids_path = train_path + (\".ids%d.en\" % en_vocabulary_size)\n",
        "  data_to_token_ids(train_path + \".hn.txt\", hn_train_ids_path, hn_vocab_path, tokenizer)\n",
        "  data_to_token_ids(train_path + \".en.txt\", en_train_ids_path, en_vocab_path, tokenizer)\n",
        "\n",
        "  # Create token ids for the development data.\n",
        "  hn_dev_ids_path = dev_path + (\".ids%d.hn\" % hn_vocabulary_size)\n",
        "  en_dev_ids_path = dev_path + (\".ids%d.en\" % en_vocabulary_size)\n",
        "  data_to_token_ids(dev_path + \".hn.txt\", hn_dev_ids_path, hn_vocab_path, tokenizer)\n",
        "  data_to_token_ids(dev_path + \".en.txt\", en_dev_ids_path, en_vocab_path, tokenizer)\n",
        "\n",
        "  return (en_train_ids_path, hn_train_ids_path,\n",
        "          en_dev_ids_path, hn_dev_ids_path,\n",
        "          en_vocab_path, hn_vocab_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSOUl2-rXJS_"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
        "import tensorflow as tf\n",
        "\n",
        "#from tensorflow import data_utils\n",
        "\n",
        "\n",
        "class Seq2SeqModel(object):\n",
        "  \"\"\"Sequence-to-sequence model with attention and for multiple buckets.\n",
        "  This class implements a multi-layer recurrent neural network as encoder,\n",
        "  and an attention-based decoder. This is the same as the model described in\n",
        "  this paper: http://arxiv.org/abs/1412.7449 - please look there for details,\n",
        "  or into the seq2seq library for complete model implementation.\n",
        "  This class also allows to use GRU cells in addition to LSTM cells, and\n",
        "  sampled softmax to handle large output vocabulary size. A single-layer\n",
        "  version of this model, but with bi-directional encoder, was presented in\n",
        "    http://arxiv.org/abs/1409.0473\n",
        "  and sampled softmax is described in Section 3 of the following paper.\n",
        "    http://arxiv.org/abs/1412.2007\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, source_vocab_size, target_vocab_size, buckets, size,\n",
        "               num_layers, max_gradient_norm, batch_size, learning_rate,\n",
        "               learning_rate_decay_factor, use_lstm=False,\n",
        "               num_samples=512, forward_only=False):\n",
        "    \"\"\"Create the model.\n",
        "    Args:\n",
        "      source_vocab_size: size of the source vocabulary.\n",
        "      target_vocab_size: size of the target vocabulary.\n",
        "      buckets: a list of pairs (I, O), where I specifies maximum input length\n",
        "        that will be processed in that bucket, and O specifies maximum output\n",
        "        length. Training instances that have inputs longer than I or outputs\n",
        "        longer than O will be pushed to the next bucket and padded accordingly.\n",
        "        We assume that the list is sorted, e.g., [(2, 4), (8, 16)].\n",
        "      size: number of units in each layer of the model.\n",
        "      num_layers: number of layers in the model.\n",
        "      max_gradient_norm: gradients will be clipped to maximimize this norm.\n",
        "      batch_size: the size of the batches used during training;\n",
        "        the model construction is independent of batch_size, so it can be\n",
        "        changed after initialization if this is convenient, e.g., for decoding.\n",
        "      learning_rate: learning rate to start with.\n",
        "      learning_rate_decay_factor: decay learning rate by this much when needed.\n",
        "      use_lstm: if true, we use LSTM cells instead of GRU cells.\n",
        "      num_samples: number of samples for sampled softmax.\n",
        "      forward_only: if set, we do not construct the backward pass in the model.\n",
        "    \"\"\"\n",
        "    self.source_vocab_size = source_vocab_size\n",
        "    self.target_vocab_size = target_vocab_size\n",
        "    self.buckets = buckets\n",
        "    self.batch_size = batch_size\n",
        "    self.learning_rate = tf.Variable(float(learning_rate), trainable=False)\n",
        "    self.learning_rate_decay_op = self.learning_rate.assign(\n",
        "        self.learning_rate * learning_rate_decay_factor)\n",
        "    self.global_step = tf.Variable(0, trainable=False)\n",
        "\n",
        "    # If we use sampled softmax, we need an output projection.\n",
        "    output_projection = None\n",
        "    softmax_loss_function = None\n",
        "    # Sampled softmax only makes sense if we sample less than vocabulary size.\n",
        "    \n",
        "    #tf.get_variable_scope().reuse_variables()\n",
        "    if num_samples > 0 and num_samples < self.target_vocab_size:\n",
        "      w = tf.get_variable(\"proj_w\", [size, self.target_vocab_size])\n",
        "      w_t = tf.transpose(w)\n",
        "      b = tf.get_variable(\"proj_b\", [self.target_vocab_size])\n",
        "      output_projection = (w, b)\n",
        "\n",
        "      def sampled_loss(inputs, labels):\n",
        "        labels = tf.reshape(labels, [-1, 1])\n",
        "        return tf.nn.sampled_softmax_loss(w_t, b, inputs, labels, num_samples,\n",
        "                self.target_vocab_size)\n",
        "      softmax_loss_function = sampled_loss\n",
        "\n",
        "    # Create the internal multi-layer cell for our RNN.\n",
        "    single_cell = tf.nn.rnn_cell.GRUCell(size)\n",
        "    if use_lstm:\n",
        "      single_cell = tf.nn.rnn_cell.BasicLSTMCell(size,state_is_tuple=True)\n",
        "    cell = single_cell\n",
        "    if num_layers > 1:\n",
        "      cell = tf.nn.rnn_cell.MultiRNNCell([single_cell] * num_layers)\n",
        "      #cell = tf.nn.rnn_cell.DropoutWrapper(cell,output_keep_prob=0.5)\n",
        "\n",
        "    # The seq2seq function: we use embedding for the input and attention.\n",
        "    def seq2seq_f(encoder_inputs, decoder_inputs, do_decode):\n",
        "      return tf.nn.seq2seq.embedding_attention_seq2seq(\n",
        "          encoder_inputs, decoder_inputs, cell,\n",
        "          num_encoder_symbols=source_vocab_size,\n",
        "          num_decoder_symbols=target_vocab_size,\n",
        "          embedding_size=size,\n",
        "          output_projection=output_projection,\n",
        "          feed_previous=do_decode)\n",
        "\n",
        "    # Feeds for inputs.\n",
        "    self.encoder_inputs = []\n",
        "    self.decoder_inputs = []\n",
        "    self.target_weights = []\n",
        "    for i in xrange(buckets[-1][0]):  # Last bucket is the biggest one.\n",
        "      self.encoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\n",
        "                                                name=\"encoder{0}\".format(i)))\n",
        "    for i in xrange(buckets[-1][1] + 1):\n",
        "      self.decoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\n",
        "                                                name=\"decoder{0}\".format(i)))\n",
        "      self.target_weights.append(tf.placeholder(tf.float32, shape=[None],\n",
        "                                                name=\"weight{0}\".format(i)))\n",
        "\n",
        "    # Our targets are decoder inputs shifted by one.\n",
        "    targets = [self.decoder_inputs[i + 1]\n",
        "               for i in xrange(len(self.decoder_inputs) - 1)]\n",
        "\n",
        "    # Training outputs and losses.\n",
        "    if forward_only:\n",
        "      self.outputs, self.losses = tf.nn.seq2seq.model_with_buckets(\n",
        "          self.encoder_inputs, self.decoder_inputs, targets,\n",
        "          self.target_weights, buckets, lambda x, y: seq2seq_f(x, y, True),\n",
        "          softmax_loss_function=softmax_loss_function)\n",
        "      # If we use output projection, we need to project outputs for decoding.\n",
        "      if output_projection is not None:\n",
        "        for b in xrange(len(buckets)):\n",
        "          self.outputs[b] = [\n",
        "              tf.matmul(output, output_projection[0]) + output_projection[1]\n",
        "              for output in self.outputs[b]\n",
        "          ]\n",
        "    else:\n",
        "      self.outputs, self.losses = tf.nn.seq2seq.model_with_buckets(\n",
        "          self.encoder_inputs, self.decoder_inputs, targets,\n",
        "          self.target_weights, buckets,\n",
        "          lambda x, y: seq2seq_f(x, y, False),\n",
        "          softmax_loss_function=softmax_loss_function)\n",
        "\n",
        "    # Gradients and SGD update operation for training the model.\n",
        "    params = tf.trainable_variables()\n",
        "    if not forward_only:\n",
        "      self.gradient_norms = []\n",
        "      self.updates = []\n",
        "      opt = tf.train.GradientDescentOptimizer(self.learning_rate)\n",
        "      for b in xrange(len(buckets)):\n",
        "        gradients = tf.gradients(self.losses[b], params)\n",
        "        clipped_gradients, norm = tf.clip_by_global_norm(gradients,\n",
        "                                                         max_gradient_norm)\n",
        "        self.gradient_norms.append(norm)\n",
        "        self.updates.append(opt.apply_gradients(\n",
        "            zip(clipped_gradients, params), global_step=self.global_step))\n",
        "\n",
        "    self.saver = tf.train.Saver(tf.all_variables())\n",
        "\n",
        "  def step(self, session, encoder_inputs, decoder_inputs, target_weights,\n",
        "           bucket_id, forward_only):\n",
        "    \"\"\"Run a step of the model feeding the given inputs.\n",
        "    Args:\n",
        "      session: tensorflow session to use.\n",
        "      encoder_inputs: list of numpy int vectors to feed as encoder inputs.\n",
        "      decoder_inputs: list of numpy int vectors to feed as decoder inputs.\n",
        "      target_weights: list of numpy float vectors to feed as target weights.\n",
        "      bucket_id: which bucket of the model to use.\n",
        "      forward_only: whether to do the backward step or only forward.\n",
        "    Returns:\n",
        "      A triple consisting of gradient norm (or None if we did not do backward),\n",
        "      average perplexity, and the outputs.\n",
        "    Raises:\n",
        "      ValueError: if length of encoder_inputs, decoder_inputs, or\n",
        "        target_weights disagrees with bucket size for the specified bucket_id.\n",
        "    \"\"\"\n",
        "    # Check if the sizes match.\n",
        "    encoder_size, decoder_size = self.buckets[bucket_id]\n",
        "    if len(encoder_inputs) != encoder_size:\n",
        "      raise ValueError(\"Encoder length must be equal to the one in bucket,\"\n",
        "                       \" %d != %d.\" % (len(encoder_inputs), encoder_size))\n",
        "    if len(decoder_inputs) != decoder_size:\n",
        "      raise ValueError(\"Decoder length must be equal to the one in bucket,\"\n",
        "                       \" %d != %d.\" % (len(decoder_inputs), decoder_size))\n",
        "    if len(target_weights) != decoder_size:\n",
        "      raise ValueError(\"Weights length must be equal to the one in bucket,\"\n",
        "                       \" %d != %d.\" % (len(target_weights), decoder_size))\n",
        "\n",
        "    # Input feed: encoder inputs, decoder inputs, target_weights, as provided.\n",
        "    input_feed = {}\n",
        "    for l in xrange(encoder_size):\n",
        "      input_feed[self.encoder_inputs[l].name] = encoder_inputs[l]\n",
        "    for l in xrange(decoder_size):\n",
        "      input_feed[self.decoder_inputs[l].name] = decoder_inputs[l]\n",
        "      input_feed[self.target_weights[l].name] = target_weights[l]\n",
        "\n",
        "    # Since our targets are decoder inputs shifted by one, we need one more.\n",
        "    last_target = self.decoder_inputs[decoder_size].name\n",
        "    input_feed[last_target] = np.zeros([self.batch_size], dtype=np.int32)\n",
        "\n",
        "    # Output feed: depends on whether we do a backward step or not.\n",
        "    if not forward_only:\n",
        "      output_feed = [self.updates[bucket_id],  # Update Op that does SGD.\n",
        "                     self.gradient_norms[bucket_id],  # Gradient norm.\n",
        "                     self.losses[bucket_id]]  # Loss for this batch.\n",
        "    else:\n",
        "      output_feed = [self.losses[bucket_id]]  # Loss for this batch.\n",
        "      for l in xrange(decoder_size):  # Output logits.\n",
        "        output_feed.append(self.outputs[bucket_id][l])\n",
        "\n",
        "    outputs = session.run(output_feed, input_feed)\n",
        "    if not forward_only:\n",
        "      return outputs[1], outputs[2], None  # Gradient norm, loss, no outputs.\n",
        "    else:\n",
        "      return None, outputs[0], outputs[1:]  # No gradient norm, loss, outputs.\n",
        "\n",
        "  def get_batch(self, data, bucket_id):\n",
        "    \"\"\"Get a random batch of data from the specified bucket, prepare for step.\n",
        "    To feed data in step(..) it must be a list of batch-major vectors, while\n",
        "    data here contains single length-major cases. So the main logic of this\n",
        "    function is to re-index data cases to be in the proper format for feeding.\n",
        "    Args:\n",
        "      data: a tuple of size len(self.buckets) in which each element contains\n",
        "        lists of pairs of input and output data that we use to create a batch.\n",
        "      bucket_id: integer, which bucket to get the batch for.\n",
        "    Returns:\n",
        "      The triple (encoder_inputs, decoder_inputs, target_weights) for\n",
        "      the constructed batch that has the proper format to call step(...) later.\n",
        "    \"\"\"\n",
        "    encoder_size, decoder_size = self.buckets[bucket_id]\n",
        "    encoder_inputs, decoder_inputs = [], []\n",
        "\n",
        "    # Get a random batch of encoder and decoder inputs from data,\n",
        "    # pad them if needed, reverse encoder inputs and add GO to decoder.\n",
        "    for _ in xrange(self.batch_size):\n",
        "      encoder_input, decoder_input = random.choice(data[bucket_id])\n",
        "\n",
        "      # Encoder inputs are padded and then reversed.\n",
        "      encoder_pad = [PAD_ID] * (encoder_size - len(encoder_input))\n",
        "      encoder_inputs.append(list(reversed(encoder_input + encoder_pad)))\n",
        "\n",
        "      # Decoder inputs get an extra \"GO\" symbol, and are padded then.\n",
        "      decoder_pad_size = decoder_size - len(decoder_input) - 1\n",
        "      decoder_inputs.append([GO_ID] + decoder_input +\n",
        "                            [PAD_ID] * decoder_pad_size)\n",
        "\n",
        "    # Now we create batch-major vectors from the data selected above.\n",
        "    batch_encoder_inputs, batch_decoder_inputs, batch_weights = [], [], []\n",
        "\n",
        "    # Batch encoder inputs are just re-indexed encoder_inputs.\n",
        "    for length_idx in xrange(encoder_size):\n",
        "      batch_encoder_inputs.append(\n",
        "          np.array([encoder_inputs[batch_idx][length_idx]\n",
        "                    for batch_idx in xrange(self.batch_size)], dtype=np.int32))\n",
        "\n",
        "    # Batch decoder inputs are re-indexed decoder_inputs, we create weights.\n",
        "    for length_idx in xrange(decoder_size):\n",
        "      batch_decoder_inputs.append(\n",
        "          np.array([decoder_inputs[batch_idx][length_idx]\n",
        "                    for batch_idx in xrange(self.batch_size)], dtype=np.int32))\n",
        "\n",
        "      # Create target_weights to be 0 for targets that are padding.\n",
        "      batch_weight = np.ones(self.batch_size, dtype=np.float32)\n",
        "      for batch_idx in xrange(self.batch_size):\n",
        "        # We set weight to 0 if the corresponding target is a PAD symbol.\n",
        "        # The corresponding target is decoder_input shifted by 1 forward.\n",
        "        if length_idx < decoder_size - 1:\n",
        "          target = decoder_inputs[batch_idx][length_idx + 1]\n",
        "        if length_idx == decoder_size - 1 or target == PAD_ID:\n",
        "          batch_weight[batch_idx] = 0.0\n",
        "      batch_weights.append(batch_weight)\n",
        "    return batch_encoder_inputs, batch_decoder_inputs, batch_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lveSwFapTUMQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 911
        },
        "outputId": "9e30f0ee-459c-46ad-c0b8-c3908f958ed6"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "import time\n",
        "import codecs\n",
        "\n",
        "import numpy as np\n",
        "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
        "import tensorflow as tf\n",
        "\"\"\"\n",
        "def del_all_flags(FLAGS):\n",
        "    flags_dict = FLAGS._flags()    \n",
        "    keys_list = [keys for keys in flags_dict]    \n",
        "    for keys in keys_list:\n",
        "        FLAGS.__delattr__(keys)\n",
        "del_all_flags(tf.flags.FLAGS)\n",
        "\"\"\"\n",
        "#tf.app.flags.DEFINE_float(\"learning_rate\", 0.001, \"Learning rate.\")\n",
        "learning_rate = 0.001\n",
        "#tf.app.flags.DEFINE_float(\"learning_rate_decay_factor\", 0.99, \"Learning rate decays by this much.\")\n",
        "learning_rate_decay_factor = 0.99\n",
        "#tf.app.flags.DEFINE_float(\"max_gradient_norm\", 5.0, \"Clip gradients to this norm.\")\n",
        "max_gradient_norm = 5.0\n",
        "#tf.app.flags.DEFINE_integer(\"batch_size\", 10, \"Batch size to use during training.\")\n",
        "batch_size = 10\n",
        "#tf.app.flags.DEFINE_integer(\"size\", 256, \"Size of each model layer.\")\n",
        "size = 256\n",
        "#tf.app.flags.DEFINE_integer(\"num_layers\", 2, \"Number of layers in the model.\")\n",
        "num_layers = 2\n",
        "#tf.app.flags.DEFINE_integer(\"en_vocab_size\", 14919, \"English vocabulary size.\")\n",
        "en_vocab_size = 14919\n",
        "#tf.app.flags.DEFINE_integer(\"hn_vocab_size\", 14919, \"Hindi vocabulary size.\")\n",
        "hn_vocab_size = 14919\n",
        "#tf.app.flags.DEFINE_string(\"data_dir\", \"/content/drive/MyDrive/DS 303 Project\", \"Data directory\")\n",
        "data_dir = \"/content/drive/MyDrive/DS 303 Project\"\n",
        "#tf.app.flags.DEFINE_string(\"transliterate_file_dir\", \"/content/drive/MyDrive/DS 303 Project\", \"Data directory\")\n",
        "transliterate_file_dir = \"/content/drive/MyDrive/DS 303 Project\"\n",
        "#tf.app.flags.DEFINE_string(\"train_dir\", \"/content/drive/MyDrive/DS 303 Project/checkpoints\", \"Training directory.\")\n",
        "train_dir = \"/content/drive/MyDrive/DS 303 Project/checkpoints\"\n",
        "#tf.app.flags.DEFINE_integer(\"max_train_data_size\", 0, \"Limit on the size of training data (0: no limit).\")\n",
        "max_train_data_size = 0\n",
        "#tf.app.flags.DEFINE_integer(\"steps_per_checkpoint\", 200, \"How many training steps to do per checkpoint.\")\n",
        "steps_per_checkpoint = 200\n",
        "#tf.app.flags.DEFINE_boolean(\"decode\", False, \"Set to True for interactive decoding.\")\n",
        "decode = False\n",
        "#tf.app.flags.DEFINE_boolean(\"transliterate_file\", False, \"Set to True for evaluating.\")\n",
        "transliterate_file = False\n",
        "#tf.app.flags.DEFINE_boolean(\"self_test\", False, \"Run a self-test if this is set to True.\")\n",
        "self_test = False\n",
        "\n",
        "#FLAGS = tf.app.flags.FLAGS\n",
        "\n",
        "# We use a number of buckets and pad to the closest one for efficiency.\n",
        "# See Seq2SeqModel for details of how they work.\n",
        "_buckets = [(3, 5), (5, 10), (10, 15), (15, 25),(25,35),(35,45)]\n",
        "\n",
        "\n",
        "\n",
        "def read_data(source_path, target_path, max_size=None):\n",
        "  \"\"\"Read data from source and target files and put into buckets.\n",
        "  Args:\n",
        "    source_path: path to the files with token-ids for the source language.\n",
        "    target_path: path to the file with token-ids for the target language;\n",
        "      it must be aligned with the source file: n-th line contains the desired\n",
        "      output for n-th line from the source_path.\n",
        "    max_size: maximum number of lines to read, all other will be ignored;\n",
        "      if 0 or None, data files will be read completely (no limit).\n",
        "  Returns:\n",
        "    data_set: a list of length len(_buckets); data_set[n] contains a list of\n",
        "      (source, target) pairs read from the provided data files that fit\n",
        "      into the n-th bucket, i.e., such that len(source) < _buckets[n][0] and\n",
        "      len(target) < _buckets[n][1]; source and target are lists of token-ids.\n",
        "  \"\"\"\n",
        "  data_set = [[] for _ in _buckets]\n",
        "  with tf.gfile.GFile(source_path, mode=\"r\") as source_file:\n",
        "    with tf.gfile.GFile(target_path, mode=\"r\") as target_file:\n",
        "      source, target = source_file.readline(), target_file.readline()\n",
        "      counter = 0\n",
        "      while source and target and (not max_size or counter < max_size):\n",
        "        counter += 1\n",
        "        if counter % 10000 == 0:\n",
        "          print(\"  reading data line %d\" % counter)\n",
        "          sys.stdout.flush()\n",
        "        source_ids = [int(x) for x in source.split()]\n",
        "        target_ids = [int(x) for x in target.split()]\n",
        "        target_ids.append(EOS_ID)\n",
        "        for bucket_id, (source_size, target_size) in enumerate(_buckets):\n",
        "          if len(source_ids) < source_size and len(target_ids) < target_size:\n",
        "            data_set[bucket_id].append([source_ids, target_ids])\n",
        "            break\n",
        "        source, target = source_file.readline(), target_file.readline()\n",
        "  return data_set\n",
        "\n",
        "\n",
        "def create_model(session, forward_only):\n",
        "  \"\"\"Create transliteration model and initialize or load parameters in session.\"\"\"\n",
        "  model = Seq2SeqModel(\n",
        "      en_vocab_size, hn_vocab_size, _buckets,\n",
        "      size, num_layers, max_gradient_norm, batch_size,\n",
        "      learning_rate, learning_rate_decay_factor,\n",
        "      forward_only=forward_only,use_lstm=False)\n",
        "  ckpt = tf.train.get_checkpoint_state(train_dir)\n",
        "  if ckpt and tf.gfile.Exists(ckpt.model_checkpoint_path):\n",
        "    print(\"Reading model parameters from %s\" % ckpt.model_checkpoint_path)\n",
        "    model.saver.restore(session, ckpt.model_checkpoint_path)\n",
        "  else:\n",
        "    print(\"Created model with fresh parameters.\")\n",
        "    session.run(tf.initialize_all_variables())\n",
        "  return model\n",
        "\n",
        "\n",
        "def train():\n",
        "  \"\"\"Train a en->hn transliteration model using REV_brandnames data.\"\"\"\n",
        "  # Prepare REV_brandnames data.\n",
        "  print(\"Preparing REV data in %s\" % data_dir)\n",
        "  en_train, hn_train, en_dev, hn_dev, _, _ = prepare_rev_data(\n",
        "      data_dir, en_vocab_size, hn_vocab_size)\n",
        "\n",
        "  with tf.Session() as sess:\n",
        "    # Create model.\n",
        "    print(\"Creating %d layers of %d units.\" % (num_layers, size))\n",
        "    model = create_model(sess, False)\n",
        "\n",
        "    # Read data into buckets and compute their sizes.\n",
        "    print (\"Reading development and training data (limit: %d).\"\n",
        "           % max_train_data_size)\n",
        "    dev_set = read_data(en_dev, hn_dev)\n",
        "    train_set = read_data(en_train, hn_train, max_train_data_size)\n",
        "    train_bucket_sizes = [len(train_set[b]) for b in xrange(len(_buckets))]\n",
        "    train_total_size = float(sum(train_bucket_sizes))\n",
        "\n",
        "    # A bucket scale is a list of increasing numbers from 0 to 1 that we'll use\n",
        "    # to select a bucket. Length of [scale[i], scale[i+1]] is proportional to\n",
        "    # the size if i-th training bucket, as used later.\n",
        "    train_buckets_scale = [sum(train_bucket_sizes[:i + 1]) / train_total_size\n",
        "                           for i in xrange(len(train_bucket_sizes))]\n",
        "\n",
        "    # This is the training loop.\n",
        "    step_time, loss = 0.0, 0.0\n",
        "    current_step = 0\n",
        "    previous_losses = []\n",
        "    while True:\n",
        "      # Choose a bucket according to data distribution. We pick a random number\n",
        "      # in [0, 1] and use the corresponding interval in train_buckets_scale.\n",
        "      random_number_01 = np.random.random_sample()\n",
        "      bucket_id = min([i for i in xrange(len(train_buckets_scale))\n",
        "                       if train_buckets_scale[i] > random_number_01])\n",
        "\n",
        "      # Get a batch and make a step.\n",
        "      start_time = time.time()\n",
        "      encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n",
        "          train_set, bucket_id)\n",
        "      _, step_loss, _ = model.step(sess, encoder_inputs, decoder_inputs,\n",
        "                                   target_weights, bucket_id, False)\n",
        "      step_time += (time.time() - start_time) / steps_per_checkpoint\n",
        "      loss += step_loss / steps_per_checkpoint\n",
        "      current_step += 1\n",
        "\n",
        "      # Once in a while, we save checkpoint, print statistics, and run evals.\n",
        "      if current_step % steps_per_checkpoint == 0:\n",
        "        # Print statistics for the previous epoch.\n",
        "        perplexity = math.exp(loss) if loss < 300 else float('inf')\n",
        "        print (\"global step %d learning rate %.4f step-time %.2f perplexity \"\n",
        "               \"%.2f\" % (model.global_step.eval(), model.learning_rate.eval(),\n",
        "                         step_time, perplexity))\n",
        "        # Decrease learning rate if no improvement was seen over last 3 times.\n",
        "        if len(previous_losses) > 2 and loss > max(previous_losses[-3:]):\n",
        "          sess.run(model.learning_rate_decay_op)\n",
        "        previous_losses.append(loss)\n",
        "        # Save checkpoint and zero timer and loss.\n",
        "        checkpoint_path = os.path.join(train_dir, \"transliterate.ckpt\")\n",
        "        model.saver.save(sess, checkpoint_path, global_step=model.global_step)\n",
        "        step_time, loss = 0.0, 0.0\n",
        "        # Run evals on development set and print their perplexity.\n",
        "        for bucket_id in xrange(len(_buckets)):\n",
        "          if len(dev_set[bucket_id]) == 0:\n",
        "            print(\"  eval: empty bucket %d\" % (bucket_id))\n",
        "            continue\n",
        "          encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n",
        "              dev_set, bucket_id)\n",
        "          _, eval_loss, _ = model.step(sess, encoder_inputs, decoder_inputs,\n",
        "                                       target_weights, bucket_id, True)\n",
        "          eval_ppx = math.exp(eval_loss) if eval_loss < 300 else float('inf')\n",
        "          print(\"  eval: bucket %d perplexity %.2f\" % (bucket_id, eval_ppx))\n",
        "        sys.stdout.flush()\n",
        "\n",
        "\n",
        "def decode():\n",
        "  with tf.Session() as sess:\n",
        "    # Create model and load parameters.\n",
        "    model = create_model(sess, True)\n",
        "    model.batch_size = 1  # We decode one word at a time.\n",
        "\n",
        "    # Load vocabularies.\n",
        "    en_vocab_path = os.path.join(data_dir,\n",
        "                                 \"vocab%d.en\" % en_vocab_size)\n",
        "    hn_vocab_path = os.path.join(data_dir,\n",
        "                                 \"vocab%d.hn\" % hn_vocab_size)\n",
        "    en_vocab, _ = initialize_vocabulary(en_vocab_path)\n",
        "    _, rev_hn_vocab = initialize_vocabulary(hn_vocab_path)\n",
        "\n",
        "    # Decode from standard input.\n",
        "    sys.stdout.write(\"> \")\n",
        "    sys.stdout.flush()\n",
        "    word = sys.stdin.readline()\n",
        "    while word:\n",
        "      word = word.lower()\n",
        "      char_list_new = list(word)\n",
        "      word = \" \".join(char_list_new)\n",
        "      # Get token-ids for the input word.\n",
        "      token_ids = word_to_token_ids(tf.compat.as_bytes(word), en_vocab)\n",
        "      # Which bucket does it belong to?\n",
        "      bucket_id = min([b for b in xrange(len(_buckets))\n",
        "                       if _buckets[b][0] > len(token_ids)])\n",
        "      # Get a 1-element batch to feed the word to the model.\n",
        "      encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n",
        "          {bucket_id: [(token_ids, [])]}, bucket_id)\n",
        "      # Get output logits for the word.\n",
        "      _, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs,\n",
        "                                       target_weights, bucket_id, True)\n",
        "      # This is a greedy decoder - outputs are just argmaxes of output_logits.\n",
        "      outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits]\n",
        "      # If there is an EOS symbol in outputs, cut them at that point.\n",
        "      if EOS_ID in outputs:\n",
        "        outputs = outputs[:outputs.index(EOS_ID)]\n",
        "      # Print out Hindi word corresponding to outputs.\n",
        "      print(\"\".join([tf.compat.as_str(rev_hn_vocab[output]) for output in outputs]))\n",
        "      print(\"> \", end=\"\")\n",
        "      sys.stdout.flush()\n",
        "      word = sys.stdin.readline()\n",
        "\n",
        "\n",
        "def self_test():\n",
        "  \"\"\"Test the translation model.\"\"\"\n",
        "  with tf.Session() as sess:\n",
        "    print(\"Self-test for neural transliteration model.\")\n",
        "    # Create model with vocabularies of 10, 2 small buckets, 2 layers of 32.\n",
        "    model = Seq2SeqModel(10, 10, [(3, 3), (6, 6)], 32, 2,\n",
        "                                       5.0, 32, 0.3, 0.99, num_samples=8)\n",
        "    sess.run(tf.initialize_all_variables())\n",
        "\n",
        "    # Fake data set for both the (3, 3) and (6, 6) bucket.\n",
        "    data_set = ([([1, 1], [2, 2]), ([3, 3], [4]), ([5], [6])],\n",
        "                [([1, 1, 1, 1, 1], [2, 2, 2, 2, 2]), ([3, 3, 3], [5, 6])])\n",
        "    for _ in xrange(5):  # Train the fake model for 5 steps.\n",
        "      bucket_id = random.choice([0, 1])\n",
        "      encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n",
        "          data_set, bucket_id)\n",
        "      model.step(sess, encoder_inputs, decoder_inputs, target_weights,\n",
        "                 bucket_id, False)\n",
        "\n",
        "def evaluate():\n",
        "  \"\"\"Generate an evaluation output of the  model.\n",
        "     Takes the directory path for evaluation from FLAGS and writes an output file to the same directory\"\"\"\n",
        "  with tf.Session() as sess:\n",
        "    # Create model and load parameters.\n",
        "    model = create_model(sess, True)\n",
        "    model.batch_size = 1  # We decode one word at a time.\n",
        "\n",
        "    # Load vocabularies.\n",
        "    en_vocab_path = os.path.join(data_dir,\n",
        "                                 \"vocab%d.en\" % en_vocab_size)\n",
        "    hn_vocab_path = os.path.join(data_dir,\n",
        "                                 \"vocab%d.hn\" % hn_vocab_size)\n",
        "    en_vocab, _ = initialize_vocabulary(en_vocab_path)\n",
        "    _, rev_hn_vocab = initialize_vocabulary(hn_vocab_path)\n",
        "\n",
        "    #path for loading the evaluation file\n",
        "    en_eval_path = os.path.join(transliterate_file_dir,'test.en')\n",
        "    print('Transliterating '+en_eval_path)\n",
        "    #Path to save the output file\n",
        "    result_path = os.path.join(transliterate_file_dir,'result.txt')\n",
        "    print('Results will be stored in '+result_path)\n",
        "\n",
        "    en_eval_list = []\n",
        "    file_content_output = []\n",
        "    print('reading input file')\n",
        "\n",
        "    with open(en_eval_path) as fp:\n",
        "      for line in fp:\n",
        "\t      char_list = list(line)\n",
        "\t      space_separated = ' '.join(char_list)\n",
        "    en_eval_list.append(space_separated)\n",
        "\n",
        "    print('decoding input file')\n",
        "    for i,word in enumerate(en_eval_list):\n",
        "      word = word.lower()\n",
        "      char_list_new = list(word)\n",
        "      word = \" \".join(char_list_new)\n",
        "      # Get token-ids for the input word.\n",
        "      token_ids = word_to_token_ids(tf.compat.as_bytes(word), en_vocab)\n",
        "      # Which bucket does it belong to?\n",
        "      bucket_list = [b for b in xrange(len(_buckets))\n",
        "                       if _buckets[b][0] > len(token_ids)]\n",
        "      #bucket_id = min([b for b in xrange(len(_buckets))\n",
        "      #                 if _buckets[b][0] > len(token_ids)])\n",
        "      if len(bucket_list) == 0:\n",
        "        print('could not find bucket')\n",
        "        continue\n",
        "      bucket_id = min(bucket_list)\n",
        "      # Get a 1-element batch to feed the word to the model.\n",
        "      encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n",
        "          {bucket_id: [(token_ids, [])]}, bucket_id)\n",
        "      # Get output logits for the word.\n",
        "      _, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs,\n",
        "                                       target_weights, bucket_id, True)\n",
        "      # This is a greedy decoder - outputs are just argmaxes of output_logits.\n",
        "      outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits]\n",
        "      # If there is an EOS symbol in outputs, cut them at that point.\n",
        "      if EOS_ID in outputs:\n",
        "        outputs = outputs[:outputs.index(EOS_ID)]\n",
        "\n",
        "      # Print out Hindi word corresponding to outputs.\n",
        "      hn_output = \"\".join([tf.compat.as_str(rev_hn_vocab[output]) for output in outputs])\n",
        "      if i%100 == 0:\n",
        "        print(str(i)+' out of ' + str(len(en_eval_list)) +' words decoded\\n English Input: ' + word + '\\t Hindi Output: ' + hn_output)\n",
        "\n",
        "      file_content_output.append([word,hn_output])\n",
        "\n",
        "    print('done generating the output file!!!')\n",
        "    fc_str = '\\n'.join(['\\t'.join(row) for row in file_content_output])\n",
        "    f = codecs.open(result_path, encoding='utf-8', mode='wb')\n",
        "    f.write(fc_str.decode('utf-8'))\n",
        "\n",
        "train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preparing REV data in /content/drive/MyDrive/DS 303 Project\n",
            "/content/drive/MyDrive/DS 303 Project\n",
            "Creating 2 layers of 256 units.\n",
            "WARNING:tensorflow:From <ipython-input-8-b20e4ab4f245>:80: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-8-b20e4ab4f245>:85: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:At least two cells provided to MultiRNNCell are the same object and will share weights.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-efc626906a43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfc_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-efc626906a43>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;31m# Create model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating %d layers of %d units.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;31m# Read data into buckets and compute their sizes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-efc626906a43>\u001b[0m in \u001b[0;36mcreate_model\u001b[0;34m(session, forward_only)\u001b[0m\n\u001b[1;32m    104\u001b[0m       \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_gradient_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate_decay_factor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m       forward_only=forward_only,use_lstm=False)\n\u001b[0m\u001b[1;32m    107\u001b[0m   \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_checkpoint_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mckpt\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-b20e4ab4f245>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, source_vocab_size, target_vocab_size, buckets, size, num_layers, max_gradient_norm, batch_size, learning_rate, learning_rate_decay_factor, use_lstm, num_samples, forward_only)\u001b[0m\n\u001b[1;32m    127\u001b[0m           ]\n\u001b[1;32m    128\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m       self.outputs, self.losses = tf.nn.seq2seq.model_with_buckets(\n\u001b[0m\u001b[1;32m    130\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuckets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/util/module_wrapper.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    191\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m       \u001b[0mattr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfmw_wrapped_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfmw_public_apis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow._api.v1.nn' has no attribute 'seq2seq'"
          ]
        }
      ]
    }
  ]
}